{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de49c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last edited 5/11/2023\n",
    "\n",
    "# PURPOSE: \n",
    "# Train a model using ARIMA and 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from datetime import timedelta\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "import keras.initializers\n",
    "from keras.layers import Dense, Layer, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3792ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THINGS TO CHANGE FOR DIFFERENT PREDICTION HORIZONS \n",
    "\n",
    "horizon = 'SCALP'\n",
    "# horizon = 'SWING'\n",
    "# horizon = 'POSITION'\n",
    "\n",
    "if horizon == 'POSITION':\n",
    "    # defined using the statistical test (but i just chose a window size)\n",
    "    n_steps = 20 #POSITION\n",
    "    n_steps_ahead = 2\n",
    "elif horizon == 'SWING':\n",
    "    n_steps = 30 #SWING, SCALP\n",
    "    n_steps_ahead = 3\n",
    "elif horizon == 'SCALP':\n",
    "    n_steps = 30\n",
    "    n_steps_ahead = 4 # forecasting horizon\n",
    "\n",
    "window_size = n_steps\n",
    "\n",
    "\n",
    "print('Horizon is {} with window size {}, forecasting n_steps ahead {}'.format(horizon, n_steps, n_steps_ahead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = 'data/'\n",
    "\n",
    "FILE_NM = 'data_1d_{}.csv'.format(horizon)\n",
    "DATA_PATH = os.path.join(BASE_PATH, DATA_DIR)\n",
    "data = os.path.join(DATA_PATH, FILE_NM)\n",
    "\n",
    "print('LOADING DATA')\n",
    "data = pd.read_csv(data, sep=\",\")[[\"prediction\", \"date\", \"close\", \"label\"]]\n",
    "print(\"Rows in df :\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000994b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data, columns=['label'], drop_first=True)\n",
    "df = df.rename(columns={\"close\": \"close\", \"label_SHORT\": \"label\"})\n",
    "# LONG = 0, SHORT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80972fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the dataset contains missing values; in order to prevent this causing errors, \n",
    "# we replace these with adjacent values from the time series.\n",
    "\n",
    "nof_missing_values = sum(np.isnan(df['close']))\n",
    "\n",
    "print(nof_missing_values, 'observations are missing.')\n",
    "print('This is {:.3f}% of the total.'.format(nof_missing_values*100/len(df)))\n",
    "\n",
    "\n",
    "# df = df.fillna(method=\"backfill\")\n",
    "# nof_missing_values = sum(np.isnan(df['USD']))\n",
    "# print('Now', nof_missing_values, 'observations are missing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d01d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the time series into training and testing sets\n",
    "# Split the training and test set by using the first 80% of the time series and the remaining \n",
    "#20% for the test set. Note that the test set must be in the future of the training set \n",
    "# to avoid look-ahead bias. Also, random sampling of the data can not be used as this would eliminate \n",
    "# the auto-correlation structure.\n",
    "\n",
    "use_features = ['close'] # continuous input\n",
    "target = ['label'] # continuous output\n",
    "# Make sure the splits are the same as 2D CNN \n",
    "# train_weight = 0.8\n",
    "# split = int(len(df) * train_weight)\n",
    "split = int(np.floor(0.8*len(df)))\n",
    "\n",
    "df_train = df[use_features].iloc[:split]\n",
    "# df_test = df[use_features].iloc[split:] \n",
    "\n",
    "df_test = df[use_features].iloc[split-n_steps:] # so the rolling predcition cv can start right at the test date for ARIMA only\n",
    "# test_label = df['label'].iloc[split:] # for ARIMA\n",
    "test_label = df['label'].iloc[split-n_steps:] # for CNN\n",
    "\n",
    "# labels (targets)\n",
    "train_label = df['label'].iloc[:split]\n",
    "# test_label = df['label'].iloc[split:]\n",
    "\n",
    "# dates\n",
    "train_date = df['prediction'].iloc[:split]\n",
    "test_date = df['prediction'].iloc[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec664f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scaling\n",
    "# Standardization of the data is important to avoid potential scaling difficulties in the fitting of the model. \n",
    "# When there is more than one feature (covariate), scaling avoids one feature dominating over another due to \n",
    "# disparate scales.\n",
    "\n",
    "# To avoid introducing a look-ahead bias into the prediction, we must re-scale the training data without \n",
    "# knowledge of the test set. Hence, we will simply standardize the training set using the mean and \n",
    "# standard deviation of the training set and not the whole time series. Additionally, to avoid introducing \n",
    "# a systematic bias into test set, we use the identical normalization for the test set - the mean and \n",
    "# standard deviation of the training set are used to normalize the test set.\n",
    "\n",
    "\n",
    "# note that for a multivariate time series, you would need to scale \n",
    "# each variable by its own mean and standard deviation in the training set\n",
    "mu = float(df_train.mean())\n",
    "sigma = float(df_train.std())\n",
    "min_ = float(df_train.min())\n",
    "max_ = float(df_train.max())\n",
    "\n",
    "normalize_input = lambda x: (x - min_) / (max_-min_)\n",
    "stdize_input = lambda x: (x - mu) / sigma\n",
    "\n",
    "# df_train = df_train.apply(stdize_input)\n",
    "df_train = df_train.apply(normalize_input)\n",
    "df_test = df_test.apply(normalize_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aedaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA\n",
    "# https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html\n",
    "\n",
    "#building the model\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "    \n",
    "train = expit(df_train['close'].to_numpy())\n",
    "test = expit(df_test['close'].to_numpy())\n",
    "    \n",
    "model = pm.auto_arima(train, \n",
    "                      max_p = n_steps, max_q = n_steps, #max lags are same as other models\n",
    "                      trace=True, error_action='ignore', \n",
    "                      step_wise=True,\n",
    "                      suppress_warnings=True,\n",
    "                      stationary=False, #is the data stationarity ?\n",
    "                      test='adf'\n",
    "                      )\n",
    "history = model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ccb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing rolling prediction\n",
    "# Set belwo to the best model found above\n",
    "\n",
    "\n",
    "# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\n",
    "\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"pmdarima version: %s\" % pm.__version__)\n",
    "\n",
    "# Load the data and split it into separate pieces\n",
    "# ARIMA(1,1,0)(0,0,0)[0] # POSITION\n",
    "# ARIMA(3,1,0)(0,0,0)[0] # SWING\n",
    "# ARIMA(2,1,1)(0,0,0)[0] # SCALP\n",
    "# y = pm.datasets.load_wineind()\n",
    "y = test\n",
    "est = pm.ARIMA(order=(2,1,1),\n",
    "               seasonal_order=(0, 0, 0, 0),\n",
    "               suppress_warnings=True)\n",
    "cv = model_selection.SlidingWindowForecastCV(window_size=n_steps, step=1, h=n_steps_ahead)\n",
    "predictions = model_selection.cross_val_predict(\n",
    "    est, y, cv=cv, verbose=2, averaging=\"mean\") #\"median\"\n",
    "\n",
    "# plot the predictions over the original series\n",
    "x_axis = np.arange(y.shape[0])\n",
    "n_test = predictions.shape[0]\n",
    "\n",
    "plt.plot(x_axis, y, alpha=0.75, c='b')\n",
    "plt.plot(x_axis[-n_test:], predictions, alpha=0.75, c='g')  # Forecasts\n",
    "plt.title(\"Cross-validated wineind forecasts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d2ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad608a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred = pd.DataFrame(np.column_stack((test_label, logit(predictions))), columns=[\"true\", \"predicted\"])\n",
    "true_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b894df",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred.to_csv('../data/true_pred_ARIMA_{}_1D.csv'.format(horizon), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3956313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras to implement a 1D convolutional neural network (CNN) for timeseries prediction.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from IPython.display import clear_output\n",
    "import keras_tuner\n",
    "from keras_tuner import Hyperband, GridSearch\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "np.set_printoptions(threshold=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796dd53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_timeseries_instances(timeseries, window_size, labels):\n",
    "    # Convert 1D vectors to 2D column vectors\n",
    "    timeseries = np.atleast_2d(timeseries)\n",
    "    if timeseries.shape[0] == 1:\n",
    "        timeseries = timeseries.T \n",
    "    \n",
    "    if not 0 < window_size < timeseries.shape[0]:\n",
    "        raise ValueError('Please set 0 < window size < timeseries length')\n",
    "    \n",
    "    # `X `is the tensor containing the inputs for the model\n",
    "    # each row of `X` is a sequence of `window_size` observations from the timeseries\n",
    "    X = [timeseries[start:start + window_size] for start in range(0, timeseries.shape[0] - window_size)]\n",
    "    \n",
    "    # for training the model, the array's dimensions must match the input layer of the CNN\n",
    "    # that is, a 3D array of shape (timeseries.shape[0] - window_size, window_size, nof_ts_variables)\n",
    "    X = np.atleast_3d(np.array(X))\n",
    "    \n",
    "    # For each row of `X`, the corresponding row of `y` is the \n",
    "    # desired output -- in this case, the subsequent value in the timeseries \n",
    "    labels = np.atleast_2d(labels)\n",
    "    if labels.shape[0] == 1:\n",
    "        labels = labels.T \n",
    "    y = labels[window_size:]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# def make_CNN(hp, window_size, filter_length,  nb_filter=4, nb_input_series=1, nb_outputs=1):\n",
    "def make_CNN(hp, window_size, nb_input_series=1, nb_outputs=1):\n",
    "    \"\"\"\n",
    "    window_size (int): number of observations in each input sequence\n",
    "    filter length (int): length of the convolutional layer's filters\n",
    "    nb_filter (int): number of filters learned in the convolutional layer\n",
    "    nb_input_series (int): number of features of the input timeseries (1 for a univariate timeseries)\n",
    "    nb_outputs (int): number of features being predicted (equal to nb_input_series \n",
    "        for predicting a timeseries at a set horizon)\n",
    "    \"\"\"\n",
    "    regularization = hp.Choice('regularization', values=[1e-4, 1e-5])\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    nb_filter = hp.Choice('nb_filter', values=[2, 3, 4, 5])\n",
    "    filter_length = hp.Choice('filter_length', values=[2, 3, 4, 5])\n",
    "    \n",
    "    model = Sequential((\n",
    "        # The convolutional layer learns `nb_filter` filters (aka kernels), \n",
    "        # each of size `(filter_length, nb_input_series)`.  \n",
    "        # Its output will have shape `(None, window_size - filter_length + 1, nb_filter)` ,  \n",
    "        # i.e., for each position in the input timeseries, the activation of each filter at that position.\n",
    "        Conv1D(filters=nb_filter, kernel_size=filter_length, activation='relu', input_shape=(window_size, nb_input_series)),\n",
    "        Flatten(),\n",
    "    #         Dense(nb_outputs, activation='sigmoid'), # For classification, a 'sigmoid' activation function would be used\n",
    "        Dense(units=nb_outputs,\n",
    "              activation='sigmoid',\n",
    "              kernel_regularizer=regularizers.L1L2(l1=regularization, l2=regularization),\n",
    "              bias_regularizer=regularizers.L2(regularization),\n",
    "              activity_regularizer=regularizers.L2(regularization))\n",
    "    ))\n",
    "    #     model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    model = make_CNN(hp, window_size, nb_input_series=1, nb_outputs=1)\n",
    "    # compile the model before training it.\n",
    "    # since there are two classes, use the tf.keras.losses.BinaryCrossentropy loss \n",
    "    # with from_logits=True since hte model provides a linear output\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=hp_learning_rate),\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy', tf.keras.metrics.BinaryCrossentropy(from_logits=True), tf.keras.metrics.AUC(from_logits=True)]\n",
    "                 )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_name(k, horizon, image_type):\n",
    "    return 'model_' + horizon + '_' + image_type + '_' + str(k) + '.h5'\n",
    "\n",
    "\n",
    "def get_vars(horizon):\n",
    "    if horizon == 'POSITION':\n",
    "        horizon_name = 'POSITION_1D'\n",
    "        splits = 3\n",
    "        BATCH_SIZE = 64\n",
    "        val_size = 150\n",
    "        max_train_size=None\n",
    "    elif horizon == 'SWING':\n",
    "        horizon_name = 'SWING_1D'\n",
    "        splits = 6\n",
    "        BATCH_SIZE = 64\n",
    "        val_size = 450\n",
    "        max_train_size=None\n",
    "    elif horizon == 'SCALP':\n",
    "        horizon_name = 'SCALP_1D'\n",
    "        splits = 10\n",
    "        BATCH_SIZE = 128\n",
    "        val_size = 1000\n",
    "        max_train_size=5000\n",
    "        \n",
    "    EPOCHS = 50    \n",
    "    image_type = '1D'\n",
    "    \n",
    "    return horizon_name, splits, BATCH_SIZE, val_size, max_train_size, EPOCHS, image_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506eb345",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_timeseries_instances(list(df_train['close']), window_size, list(train_label))\n",
    "x_train = X\n",
    "y_train = y\n",
    "\n",
    "#SET HERE\n",
    "# # horizon='POSITION_1D'\n",
    "# horizon='SWING_1D'\n",
    "# image_type = '1D'\n",
    "\n",
    "# EPOCHS = 50\n",
    "# BATCH_SIZE = 64 \n",
    "# splits = 6 # 3 #\n",
    "# val_size = 450 #150 #\n",
    "\n",
    "horizon, splits, BATCH_SIZE, val_size, max_train_size, EPOCHS, image_type = get_vars(horizon)\n",
    "\n",
    "print('Horizon: {}, Splits: {}, BATCH_SIZE: {}, val_size: {}, max_train_size:{}, EPOCHS: {}, image_type: {}'.format(horizon, splits, BATCH_SIZE, val_size, max_train_size, EPOCHS, image_type))\n",
    "\n",
    "print('length of X: {}, length of y: {}'.format(len(X), len(y)))\n",
    "print('X:', X, 'y:', y, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bf90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if horizon == 'SCALP_1D':\n",
    "    training_size = 15000\n",
    "    x_train = x_train[-training_size:]\n",
    "    y_train = y_train[-training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMBINE & EDIT\n",
    "PATH = os.path.dirname(os.getcwd())\n",
    "save_dir = PATH + '/saved_models/{}/'.format(horizon)\n",
    "#Time-series split\n",
    "# tscv = TimeSeriesSplit(n_splits=splits, test_size=val_size,  max_train_size= max_train_size)\n",
    "tscv = TimeSeriesSplit(n_splits=splits, test_size=val_size)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(tscv.split(x_train)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    \n",
    "    VALIDATION_ACCURACY = []\n",
    "    VALIDATION_LOSS = []\n",
    "    BEST_HYPS = []\n",
    "    \n",
    "    train_index = train_index[i*val_size:]\n",
    "    if i < 9:\n",
    "        continue\n",
    "    df_train_x = x_train[train_index]\n",
    "    df_train_y = y_train[train_index]\n",
    "    \n",
    "    df_val_x = x_train[val_index]\n",
    "    df_val_y = y_train[val_index]\n",
    "    \n",
    "    print(\"train : {}\".format(len(df_train_x)))\n",
    "    print(\"validation : {}\".format(len(df_val_x)))\n",
    "    steps_per_epoch = len(df_train_x) // BATCH_SIZE\n",
    "    validation_steps = len(df_val_x)// BATCH_SIZE\n",
    "    \n",
    "    # Create Callbacks\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(i, horizon, image_type),\n",
    "                                                  monitor='val_accuracy', verbose=1,\n",
    "                                                  save_best_only=True, mode='max')\n",
    "    \n",
    "    # create a callback to stop training early after reaching a certain value for the validation loss\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3,  min_delta=0.001, verbose=1, restore_best_weights=True, start_from_epoch=3)\n",
    "    \n",
    "    # https://keras.io/api/keras_tuner/tuners/base_tuner/#tuner-class\n",
    "    tuner = keras_tuner.GridSearch(\n",
    "        hypermodel = build_model,\n",
    "        objective=\"val_accuracy\",\n",
    "        seed=5,\n",
    "        max_trials=100,\n",
    "        overwrite=True,\n",
    "        directory=\"model_{}_{}\".format(horizon, image_type),\n",
    "        project_name=\"tune_hypermodel_{}\".format(i),)\n",
    "    \n",
    "    tuner.search(df_train_x, df_train_y, \n",
    "                 epochs=EPOCHS, \n",
    "                 steps_per_epoch=steps_per_epoch,\n",
    "                 validation_data=(df_val_x, df_val_y), \n",
    "                 validation_steps=validation_steps,\n",
    "                 callbacks=[stop_early])\n",
    "        # Get the optimal hyperparameters\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    BEST_HYPS.append(best_hps.values)\n",
    "    \n",
    "    # Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    history = model.fit(df_train_x, df_train_y, \n",
    "                        epochs=EPOCHS,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=(df_val_x, df_val_y), \n",
    "                        validation_steps=validation_steps,\n",
    "                        callbacks=[checkpoint, stop_early])\n",
    "\n",
    "    # Load best model to evaluate performance\n",
    "    model.load_weights(os.path.join(save_dir, \"model_\"+ horizon + '_' + image_type + '_' + str(i) + \".h5\"))\n",
    "    scores = model.evaluate(df_val_x, df_val_y)\n",
    "    print(\"{0}s: {1:.2f}%\".format(model.metrics_names[1], scores[1]*100))\n",
    "    results = dict(zip(model.metrics_names, scores))\n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "    \n",
    "    dict_results = {'VALIDATION_ACCURACY': VALIDATION_ACCURACY, \n",
    "                    'VALIDATION_LOSS': VALIDATION_LOSS,\n",
    "                    'BEST_HYPS': BEST_HYPS}\n",
    "    df = pd.DataFrame(dict_results)\n",
    "    df.to_csv('model_{0}_{1}/results_{0}_{1}_fold{2}.csv'.format(horizon, image_type, i))\n",
    "\n",
    "    # Good practice to explicitly close each \n",
    "    # tensorflow session prior to starting a \n",
    "    # new one in a loop for memory considerations\n",
    "    tf.keras.backend.clear_session()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13584486",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for split in range(splits):\n",
    "    result = pd.read_csv('model_{0}_{1}/results_{0}_{1}_fold{2}.csv'.format(horizon, image_type, split))[['VALIDATION_ACCURACY', 'VALIDATION_LOSS', 'BEST_HYPS']]\n",
    "    frames.append(list(result.iloc[0]))\n",
    "results_df = pd.DataFrame(frames, columns=['VALIDATION_ACCURACY', 'VALIDATION_LOSS', 'BEST_HYPS'])    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d18da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c91bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_csv('./results_{}_{}.csv'.format(horizon, image_type))[['VALIDATION_ACCURACY', 'VALIDATION_LOSS', 'BEST_HYPS']]\n",
    "\n",
    "VALIDATION_ACCURACY = list(results_df.VALIDATION_ACCURACY)\n",
    "VALIDATION_LOSS = list(results_df.VALIDATION_LOSS)\n",
    "BEST_HYPS = list(results_df.BEST_HYPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ce068",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = make_timeseries_instances(list(df_test['close']), window_size, list(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440860ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "\n",
    "best_model_index = VALIDATION_ACCURACY.index(max(VALIDATION_ACCURACY))\n",
    "# load and evaluate a saved model\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# load model\n",
    "# https://keras.io/guides/serialization_and_saving/\n",
    "# model = load_model(save_dir + 'model_{}_{}{}.h5'.format(horizon, image_type, best_model_index), custom_objects={'mda': mda})\n",
    "model = load_model(save_dir + 'model_{}_{}_{}.h5'.format(horizon, image_type, best_model_index))\n",
    "\n",
    "# summarize model.\n",
    "model.summary()\n",
    "\n",
    "# evaluate the model\n",
    "# score = model.evaluate(test_generator, verbose=0)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_HYPS[best_model_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260369f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "print(\"{}: {}%\".format(model.metrics_names[0], score[0]*100))\n",
    "print(\"{}: {}%\".format(model.metrics_names[1], score[1]*100))\n",
    "print(\"{}: {}%\".format(model.metrics_names[2], score[2]*100))\n",
    "print(\"{}: {}%\".format(model.metrics_names[3], score[3]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26babcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making predictions with the model\n",
    "# Get the predicted values for the test set:\n",
    "# test_generator.filenames #LONG = 0, SHORT = 1\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a93a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.column_stack((y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae456316",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred = pd.DataFrame(np.column_stack((y_test, y_pred)), columns=[\"true\", \"predicted\"])\n",
    "true_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred.to_csv('../data/true_pred_2dCNN_{}_{}.csv'.format(horizon, image_type), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
