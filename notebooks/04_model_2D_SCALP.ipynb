{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d8f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Used/Edited: 5/11/2023\n",
    "\n",
    "# PURPOSE: \n",
    "# Train a model using the generated images for SCALP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1365d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from typing import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import datetime as dt\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras import datasets,models,layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "import keras_tuner\n",
    "from keras_tuner import Hyperband, GridSearch\n",
    "# Tune model training\n",
    "# https://keras.io/guides/keras_tuner/getting_started/\n",
    "\n",
    "# It is generally not needed to tune the number of epochs because a built-in callback is \n",
    "# passed to model.fit() to save the model at its best epoch evaluated by the validation_data.\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 2\n",
    "tf.random.set_seed(seed)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcb810",
   "metadata": {},
   "source": [
    "## Set Image Type Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f678cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET HERE\n",
    "horizon = 'SCALP'\n",
    "# horizon='SCALP_ST'\n",
    "\n",
    "# image_type = 'GAF'\n",
    "# image_type = 'GAF_AGG'\n",
    "image_type = 'CS'\n",
    "# image_type = 'TI'\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "splits = 10\n",
    "max_train_size=5000\n",
    "val_size = 1000\n",
    "training_size = 15000\n",
    "#Time-series split\n",
    "tscv = TimeSeriesSplit(n_splits=splits, max_train_size=max_train_size, test_size=val_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df4bd7",
   "metadata": {},
   "source": [
    "## Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9609814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_paths_labels(path):\n",
    "    dataframes = []\n",
    "    for sub_folder in ['LONG', 'SHORT']:\n",
    "        images = glob.glob(path + '/{}/*.png'.format(sub_folder))  # Get path to images\n",
    "        dates = [dt.split('/')[-1].split('\\\\')[-1].split('.')[0].replace('_', '-') for dt in images]\n",
    "        data_slice = pd.DataFrame({'Images': images, 'Labels': [sub_folder] * len(images), 'Dates': dates})\n",
    "        data_slice['Dates'] = pd.to_datetime(data_slice['Dates'], format='%Y-%m-%d-%H-%M-%S')\n",
    "        dataframes.append(data_slice)\n",
    "    data = pd.concat(dataframes)\n",
    "    data.sort_values(by='Dates', inplace=True)\n",
    "    del data['Dates']\n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_model_name(k, horizon, image_type):\n",
    "    return 'model_' + horizon + '_' + image_type + '_' + str(k) + '.h5'\n",
    "\n",
    "\n",
    "def myModel(hp, tg, IMG_SIZE):\n",
    "    IMG_SHAPE = IMG_SIZE + (3,)\n",
    "    # tf.keras.applications.MobileNetV2\n",
    "    # tf.keras.applications.resnet_v2.ResNet50V2\n",
    "    base_model = tf.keras.applications.resnet_v2.ResNet50V2(input_shape=IMG_SHAPE,\n",
    "                                                  include_top=False,\n",
    "                                                  weights='imagenet')\n",
    "\n",
    "    # This feature extractor converts each 160x160x3 image into a 5x5x1280 block of features\n",
    "    # lets see what it does to an example batch of images\n",
    "\n",
    "    image_batch, label_batch = next(iter(tg))\n",
    "    feature_batch = base_model(image_batch)\n",
    "#     print(feature_batch.shape)\n",
    "\n",
    "    # Feature extraction\n",
    "    # in this step, you will freeze the convolutional base created from the prev\n",
    "    # step and to use as a feature extractor.\n",
    "    # additionally, you add a classifier on top of it and train the top-level classifier\n",
    "\n",
    "    # Freeze the convolutional base\n",
    "    # it is important to freeze the convolutional base before you compile and train the model.\n",
    "    # freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training.\n",
    "    # MobileNet V2 has many layers, so setting the entire model's trainable flag False will freeze all of them\n",
    "\n",
    "    base_model.trainable = False\n",
    "#     base_model.trainable = True\n",
    "\n",
    "    # base model architecture\n",
    "#     base_model.summary()\n",
    "\n",
    "    # Add a classification head\n",
    "\n",
    "    # to generate from the block of features, average over the spatial 5x5 location,\n",
    "    # using tf.keras.layers.GloabalAveragePooling2D layer to convert the features to a single 1280-element vector per image\n",
    "\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    feature_batch_average = global_average_layer(feature_batch)\n",
    "#     print(feature_batch_average.shape)\n",
    "\n",
    "    # apply tf.keras.layers.Dense layer to convert these features into a single prediction per image\n",
    "    # you dont need an activation function here because this prediction will be treated as a logit, or a raw prediction value\n",
    "    # positive numbers predict class 1, negative numbers predict class 0\n",
    "    regularization = hp.Choice('regularization', values=[1e-4, 1e-5])\n",
    "    prediction_layer = tf.keras.layers.Dense(units=1,\n",
    "                                             kernel_regularizer=regularizers.L1L2(l1=regularization, l2=regularization),\n",
    "                                             bias_regularizer=regularizers.L2(regularization),\n",
    "                                             activity_regularizer=regularizers.L2(regularization))\n",
    "    prediction_batch = prediction_layer(feature_batch_average)\n",
    "\n",
    "    # build a model by chaining together the data augmentation, rescaling, base_model, and feature\n",
    "    # extractor layers using Keras Functional API\n",
    "    # as prev mentioned, use training=False b/c model contains a BatchNormalization layer\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(255, 255, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = global_average_layer(x)\n",
    "    \n",
    "    drop_out_rate = hp.Float(\"drop_out_rate\", min_value=0, max_value=1, step=0.2)\n",
    "    x = tf.keras.layers.Dropout(drop_out_rate)(x)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    model = myModel(hp, train_generator, (255, 255))\n",
    "    # compile the model before training it.\n",
    "    # since there are two classes, use the tf.keras.losses.BinaryCrossentropy loss \n",
    "    # with from_logits=True since hte model provides a linear output\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=hp_learning_rate),\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy', tf.keras.metrics.BinaryCrossentropy(from_logits=True), tf.keras.metrics.AUC(from_logits=True)])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749d518",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2762017",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.dirname(os.getcwd())\n",
    "IMAGES_PATH = os.path.join(PATH, 'TRAIN/{}/{}'.format(horizon, image_type))\n",
    "X = image_paths_labels(IMAGES_PATH)\n",
    "\n",
    "# first split train, test\n",
    "split_ratio = int(np.floor(0.8*len(X)))\n",
    "train = X[:split_ratio]\n",
    "train = train[-training_size:]\n",
    "test = X[split_ratio:]\n",
    "\n",
    "# Rescale images by 1./255\n",
    "train_validate_datagen = ImageDataGenerator(rescale=1/255)#, validation_split=0.3) # set validation split\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "IMG_SIZE = (255, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc63ed",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9835f273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Complete [00h 11m 45s]\n",
      "val_accuracy: 0.5133928656578064\n",
      "\n",
      "Best val_accuracy So Far: 0.5145089626312256\n",
      "Total elapsed time: 01h 24m 37s\n",
      "\n",
      "Search: Running Trial #8\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.0001            |0.0001            |regularization\n",
      "0.4               |0                 |drop_out_rate\n",
      "0.001             |0.001             |learning_rate\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 11:40:12.186718: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - ETA: 0s - loss: 0.7603 - accuracy: 0.4918 - binary_crossentropy: 0.7552 - auc: 0.5027"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 11:41:26.372508: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-09 11:41:45.435180: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at save_restore_v2_ops.cc:138 : RESOURCE_EXHAUSTED: model_SCALP_ST_CS/tune_hypermodel_5/trial_07/checkpoint_temp/part-00000-of-00001.data-00000-of-00001.tempstate6544457628041552953; No space left on device\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rurikoimai/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/Users/rurikoimai/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/Users/rurikoimai/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"/Users/rurikoimai/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/engine/tuner.py\", line 214, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "  File \"/Users/rurikoimai/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/engine/hypermodel.py\", line 144, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "  File \"/Users/rurikoimai/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/Users/rurikoimai/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __wrapped__SaveV2_dtypes_282_device_/job:localhost/replica:0/task:0/device:CPU:0}} model_SCALP_ST_CS/tune_hypermodel_5/trial_07/checkpoint_temp/part-00000-of-00001.data-00000-of-00001.tempstate6544457628041552953; No space left on device [Op:SaveV2]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "model_SCALP_ST_CS/tune_hypermodel_5/trial_07/trial.json; No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 72\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# https://keras.io/api/keras_tuner/tuners/base_tuner/#tuner-class\u001b[39;00m\n\u001b[1;32m     63\u001b[0m tuner \u001b[38;5;241m=\u001b[39m keras_tuner\u001b[38;5;241m.\u001b[39mGridSearch(\n\u001b[1;32m     64\u001b[0m     hypermodel \u001b[38;5;241m=\u001b[39m build_model,\n\u001b[1;32m     65\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(horizon, image_type),\n\u001b[1;32m     70\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtune_hypermodel_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i),)\n\u001b[0;32m---> 72\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m             \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m             \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m             \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m             \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstop_early\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Get the optimal hyperparameters\u001b[39;00m\n\u001b[1;32m     80\u001b[0m best_hps\u001b[38;5;241m=\u001b[39mtuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:231\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:335\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# Display needs the updated trial scored by the Oracle.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display\u001b[38;5;241m.\u001b[39mon_trial_end(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/engine/oracle.py:107\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    106\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[0;32m--> 107\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[1;32m    109\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/tuners/gridsearch.py:318\u001b[0m, in \u001b[0;36mGridSearchOracle.end_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;129m@oracle_module\u001b[39m\u001b[38;5;241m.\u001b[39msynchronized\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mend_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# It is OK for a trial_id to be pushed into _populate_next multiple\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# times. It will be skipped during _populate_space if its next\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# combination has been tried.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# For not blocking _populate_space, we push it regardless of the status.\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_populate_next\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/engine/oracle.py:107\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    106\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[0;32m--> 107\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[1;32m    109\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/engine/oracle.py:436\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_consecutive_failures()\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/engine/oracle.py:640\u001b[0m, in \u001b[0;36mOracle._save_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;66;03m# Write trial status to trial directory\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     trial_id \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mtrial_id\n\u001b[0;32m--> 640\u001b[0m     \u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trial_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrial.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/keras_tuner/engine/stateful.py:68\u001b[0m, in \u001b[0;36mStateful.save\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m     66\u001b[0m state_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(state)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(fname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 68\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(fname)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/tensorflow/python/lib/io/file_io.py:100\u001b[0m, in \u001b[0;36mFileIO.write\u001b[0;34m(self, file_content)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_content):\n\u001b[1;32m     99\u001b[0m   \u001b[38;5;124;03m\"\"\"Writes file_content to the file. Appends to the end of the file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prewrite_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writable_file\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    102\u001b[0m       compat\u001b[38;5;241m.\u001b[39mas_bytes(file_content, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__encoding))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_thesis/lib/python3.9/site-packages/tensorflow/python/lib/io/file_io.py:85\u001b[0m, in \u001b[0;36mFileIO._prewrite_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_check_passed:\n\u001b[1;32m     83\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mPermissionDeniedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt open for writing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writable_file \u001b[38;5;241m=\u001b[39m \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWritableFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__mode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: model_SCALP_ST_CS/tune_hypermodel_5/trial_07/trial.json; No space left on device"
     ]
    }
   ],
   "source": [
    "### COMBINE & EDIT\n",
    "\n",
    "save_dir = PATH + '/saved_models/{}/'.format(horizon)\n",
    "TIMESTAMP = dt.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(tscv.split(train)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    \n",
    "    VALIDATION_ACCURACY = []\n",
    "    VALIDATION_LOSS = []\n",
    "    BEST_HYPS = []\n",
    "\n",
    "#     if i < 1:\n",
    "#         continue\n",
    "    df_train = train.iloc[train_index]\n",
    "    df_val = train.iloc[val_index]\n",
    "\n",
    "    train_generator = train_validate_datagen.flow_from_dataframe(\n",
    "        dataframe=df_train,\n",
    "        directory=IMAGES_PATH,\n",
    "        target_size=IMG_SIZE,\n",
    "        x_col='Images',\n",
    "        y_col='Labels',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        class_mode='binary',\n",
    "    )\n",
    "    \n",
    "    validation_generator = train_validate_datagen.flow_from_dataframe(\n",
    "        dataframe=df_val,\n",
    "        directory=IMAGES_PATH,\n",
    "        target_size=IMG_SIZE,\n",
    "        x_col='Images',\n",
    "        y_col='Labels',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        class_mode='binary',\n",
    "    )\n",
    "\n",
    "    print(\"train datagenerator : {}\".format(train_generator.n))\n",
    "    print(\"validation datagenerator : {}\".format(validation_generator.n))\n",
    "    steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    validation_steps = validation_generator.n//validation_generator.batch_size # if you have validation data \n",
    "    \n",
    "    \n",
    "    # Create Callbacks\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(i, horizon, image_type),\n",
    "                                                  monitor='val_accuracy', verbose=1,\n",
    "                                                  save_best_only=True, mode='max')\n",
    "    \n",
    "    # create a callback to stop training early after reaching a certain value for the validation loss\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3,  min_delta=0.001, verbose=1, restore_best_weights=True, start_from_epoch=3)\n",
    "    \n",
    "    # https://keras.io/api/keras_tuner/tuners/base_tuner/#tuner-class\n",
    "    tuner = keras_tuner.GridSearch(\n",
    "        hypermodel = build_model,\n",
    "        objective=\"val_accuracy\",\n",
    "        seed=5,\n",
    "        max_trials=10,\n",
    "        overwrite=True,\n",
    "        directory=\"model_{}_{}\".format(horizon, image_type),\n",
    "        project_name=\"tune_hypermodel_{}\".format(i),)\n",
    "    \n",
    "    tuner.search(train_generator, \n",
    "                 epochs=EPOCHS, \n",
    "                 steps_per_epoch=steps_per_epoch,\n",
    "                 validation_data=validation_generator, \n",
    "                 validation_steps=validation_steps,\n",
    "                 callbacks=[stop_early])\n",
    "\n",
    "    # Get the optimal hyperparameters\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    BEST_HYPS.append(best_hps.values)\n",
    "    print(f\"\"\"\n",
    "    The hyperparameter search is complete. The optimal regularization is {best_hps.get('regularization')},the optimal learning rate for the optimizer\n",
    "    is {best_hps.get('learning_rate')}, and the optimal drop out rate is {best_hps.get('drop_out_rate')}.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    history = model.fit(train_generator, \n",
    "                        epochs=EPOCHS,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=validation_generator, \n",
    "                        validation_steps=validation_steps,\n",
    "                        callbacks=[checkpoint, stop_early])\n",
    "\n",
    "    # Load best model to evaluate performance\n",
    "    model.load_weights(os.path.join(save_dir, \"model_\"+ horizon + '_' + image_type + '_' + str(i) + \".h5\"))\n",
    "    scores = model.evaluate(validation_generator)\n",
    "    print(\"{0}s: {1:.2f}%\".format(model.metrics_names[1], scores[1]*100))\n",
    "    results = dict(zip(model.metrics_names, scores))\n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "\n",
    "    dict_results = {'VALIDATION_ACCURACY': VALIDATION_ACCURACY, \n",
    "                    'VALIDATION_LOSS': VALIDATION_LOSS,\n",
    "                    'BEST_HYPS': BEST_HYPS}\n",
    "    df = pd.DataFrame(dict_results)\n",
    "    df.to_csv('model_{0}_{1}/results_{0}_{1}_fold{2}.csv'.format(horizon, image_type, i))\n",
    "    \n",
    "    # Good practice to explicitly close each \n",
    "    # tensorflow session prior to starting a \n",
    "    # new one in a loop for memory considerations\n",
    "    tf.keras.backend.clear_session()\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ffc908",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for split in range(splits):\n",
    "    result = pd.read_csv('model_{0}_{1}/results_{0}_{1}_fold{2}.csv'.format(horizon, image_type, split))[['VALIDATION_ACCURACY', 'VALIDATION_LOSS', 'BEST_HYPS']]\n",
    "    frames.append(list(result.iloc[0]))\n",
    "results_df = pd.DataFrame(frames, columns = ['VALIDATION_ACCURACY', 'VALIDATION_LOSS', 'BEST_HYPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d4294",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_ACCURACY = list(results_df.VALIDATION_ACCURACY)\n",
    "VALIDATION_LOSS = list(results_df.VALIDATION_LOSS)\n",
    "BEST_HYPS = list(results_df.BEST_HYPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5eff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "\n",
    "best_model_index = VALIDATION_ACCURACY.index(max(VALIDATION_ACCURACY))\n",
    "# load and evaluate a saved model\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# load model\n",
    "# https://keras.io/guides/serialization_and_saving/\n",
    "# model = load_model(save_dir + 'model_{}_{}{}.h5'.format(horizon, image_type, best_model_index), custom_objects={'mda': mda})\n",
    "model = load_model(save_dir + 'model_{}_{}_{}.h5'.format(horizon, image_type, best_model_index))\n",
    "\n",
    "# summarize model.\n",
    "model.summary()\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test,\n",
    "    x_col='Images',\n",
    "    y_col='Labels',\n",
    "    directory=IMAGES_PATH,\n",
    "    target_size=IMG_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "score = model.evaluate(test_generator, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_HYPS[best_model_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "print(\"{}: {}%\".format(model.metrics_names[0], score[0]*100))\n",
    "print(\"{}: {}%\".format(model.metrics_names[1], score[1]*100))\n",
    "print(\"{}: {}%\".format(model.metrics_names[2], score[2]*100))\n",
    "print(\"{}: {}%\".format(model.metrics_names[3], score[3]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d74e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making predictions with the model\n",
    "# Get the predicted values for the test set:\n",
    "# test_generator.filenames #LONG = 0, SHORT = 1\n",
    "\n",
    "y_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred = pd.DataFrame(np.column_stack((test_generator.labels, y_pred)), columns=[\"true\", \"predicted\"])\n",
    "true_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51843d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred.to_csv('../data/true_pred_2dCNN_{}_{}.csv'.format(horizon, image_type), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
