{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb2780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last used/edited: 5/8/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6339497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: \n",
    "# Generate Stationary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ed93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. use preprocess data and resample by frequency to check for stationarity\n",
    "#2. use the pkl data to difference found in step 1\n",
    "#3. back fill NaN values \n",
    "#4. output files as stationary_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. use 1D data for each freq and fit arima model to get stationarity num\n",
    "\n",
    "# Load data\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = 'data/'\n",
    "# horizon = 'SCALP'\n",
    "# horizon = 'SWING'\n",
    "# horizon = 'POSITION'\n",
    "\n",
    "#DATA\n",
    "#SCALP\n",
    "# data_1d_SCALP_df0.csv\n",
    "# data_1d_SCALP_df1.csv\n",
    "# data_1d_SCALP_df2.csv\n",
    "\n",
    "#SWING\n",
    "# data_1d_SWING_freqIdx0_CHECKSTATIONARY.csv\n",
    "# data_1d_SWING_freqIdx1_CHECKSTATIONARY.csv\n",
    "# data_1d_SWING_freqIdx2_CHECKSTATIONARY.csv\n",
    "\n",
    "#POSITION\n",
    "# data_1d_POSITION_freqIdx0_CHECKSTATIONARY.csv\n",
    "# data_1d_POSITION_freqIdx1_CHECKSTATIONARY.csv\n",
    "# data_1d_POSITION_freqIdx2_CHECKSTATIONARY.csv\n",
    "\n",
    "FILE_NM = 'data_1d_SWING_freqIdx1_CHECKSTATIONARY.csv'\n",
    "DATA_PATH = os.path.join(BASE_PATH, DATA_DIR)\n",
    "data = os.path.join(DATA_PATH, FILE_NM)\n",
    "\n",
    "print('LOADING DATA')\n",
    "data = pd.read_csv(data, sep=\",\")[[\"prediction\", \"date\", \"close\", \"label\"]]\n",
    "print(\"Rows in df :\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data, columns=['label'], drop_first=True)\n",
    "df = df.rename(columns={\"close\": \"close\", \"label_SHORT\": \"label\"})\n",
    "# LONG = 0, SHORT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d8b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined using the statistical test (but i just chose a window size)\n",
    "n_steps = 30 #SWING, SCALP\n",
    "# n_steps = 20 #POSITION\n",
    "\n",
    "use_features = ['close'] # continuous input\n",
    "target = ['label'] # continuous output\n",
    "n_steps_ahead = 3 #2 #3 #4  # forecasting horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca216e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the time series into training and testing sets\n",
    "# Split the training and test set by using the first 80% of the time series and the remaining \n",
    "#20% for the test set. Note that the test set must be in the future of the training set \n",
    "# to avoid look-ahead bias. Also, random sampling of the data can not be used as this would eliminate \n",
    "# the auto-correlation structure.\n",
    "\n",
    "\n",
    "# Make sure the splits are the same as 2D CNN \n",
    "# train_weight = 0.8\n",
    "# split = int(len(df) * train_weight)\n",
    "split = int(np.floor(0.8*len(df)))\n",
    "\n",
    "df_train = df[use_features].iloc[:split]\n",
    "# df_test = df[use_features].iloc[split:] \n",
    "#ARIMA change this\n",
    "df_test = df[use_features].iloc[split-n_steps:] # so the rolling predcition cv can start right at the test date for ARIMA only\n",
    "\n",
    "# labels (targets)\n",
    "train_label = df['label'].iloc[:split]\n",
    "test_label = df['label'].iloc[split:]\n",
    "\n",
    "# dates\n",
    "train_date = df['prediction'].iloc[:split]\n",
    "test_date = df['prediction'].iloc[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f66f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scaling\n",
    "# Standardization of the data is important to avoid potential scaling difficulties in the fitting of the model. \n",
    "# When there is more than one feature (covariate), scaling avoids one feature dominating over another due to \n",
    "# disparate scales.\n",
    "\n",
    "# To avoid introducing a look-ahead bias into the prediction, we must re-scale the training data without \n",
    "# knowledge of the test set. Hence, we will simply standardize the training set using the mean and \n",
    "# standard deviation of the training set and not the whole time series. Additionally, to avoid introducing \n",
    "# a systematic bias into test set, we use the identical normalization for the test set - the mean and \n",
    "# standard deviation of the training set are used to normalize the test set.\n",
    "\n",
    "\n",
    "# note that for a multivariate time series, you would need to scale \n",
    "# each variable by its own mean and standard deviation in the training set\n",
    "mu = float(df_train.mean())\n",
    "sigma = float(df_train.std())\n",
    "min_ = float(df_train.min())\n",
    "max_ = float(df_train.max())\n",
    "\n",
    "normalize_input = lambda x: (x - min_) / (max_-min_)\n",
    "stdize_input = lambda x: (x - mu) / sigma\n",
    "\n",
    "# df_train = df_train.apply(stdize_input)\n",
    "df_train = df_train.apply(normalize_input)\n",
    "df_test = df_test.apply(normalize_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8adf83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA\n",
    "# https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html\n",
    "\n",
    "#building the model\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "train = df_train['close'].to_numpy()\n",
    "\n",
    "model = pm.auto_arima(train, \n",
    "                      max_p = n_steps, max_q = n_steps, #max lags are same as other models\n",
    "                      trace=True, error_action='ignore', \n",
    "                      step_wise=True,\n",
    "                      suppress_warnings=True,\n",
    "                      stationary=False, #is the data stationarity ?\n",
    "                      test='adf'\n",
    "                      )\n",
    "model.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f932b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECORD the models here\n",
    "\n",
    "#SCALP\n",
    "#freq=0 ARIMA(2,1,1)(0,0,0)[0]\n",
    "#freq=1 ARIMA(0,1,1)(0,0,0)[0]  \n",
    "#freq=2 ARIMA(2,1,1)(0,0,0)[0] \n",
    "\n",
    "#SWING\n",
    "#freq=0 ARIMA(0,1,0)(0,0,0)[0] \n",
    "#freq=1 ARIMA(1,1,0)(0,0,0)[0]\n",
    "#freq=2 ARIMA(1,1,0)(0,0,0)[0]  \n",
    "\n",
    "#POSITION\n",
    "#freq=0 ARIMA(0,1,0)(0,0,0)[0]    \n",
    "#freq=1 ARIMA(0,1,0)(0,0,0)[0]  \n",
    "#freq=2 ARIMA(0,1,0)(0,0,0)[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d04dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. use the pkl data to difference found in step 1\n",
    "#3. back fill NaN values \n",
    "#4. output files as stationary_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff5a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_diff_fill(data, diff_by = 1, fill_with = 0.0):\n",
    "    diffed = data.diff(diff_by)\n",
    "    diffed.iloc[0] = fill_with\n",
    "    return diffed\n",
    "\n",
    "\n",
    "def create_technical_indicator_data(label, dict_):\n",
    "    \n",
    "    for idx in range(len(dict_[label])):\n",
    "        image_name = dict_[label][idx][0]\n",
    "        image_name='{0}.png'.format(image_name.replace('-','_').replace(' ','_').replace(':','_'))\n",
    "        print(image_name)\n",
    "\n",
    "        for freq in range(4):\n",
    "            high = dict_[label][idx][1]['High'][freq]\n",
    "            close = dict_[label][idx][1]['Close'][freq]\n",
    "            low = dict_[label][idx][1]['Low'][freq]\n",
    "            open_ = dict_[label][idx][1]['Open'][freq]\n",
    "\n",
    "            dict_[label][idx][1]['High'][freq] = take_diff_fill(high, diff_by = 1, fill_with = 0.0)\n",
    "            dict_[label][idx][1]['Close'][freq] = take_diff_fill(close, diff_by = 1, fill_with = 0.0)\n",
    "            dict_[label][idx][1]['Low'][freq] = take_diff_fill(low, diff_by = 1, fill_with = 0.0)\n",
    "            dict_[label][idx][1]['Open'][freq] = take_diff_fill(open_, diff_by = 1, fill_with = 0.0)\n",
    "\n",
    "    return dict_[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05039eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(PATH, 'data')\n",
    "\n",
    "#SCALP\n",
    "# file_name = 'W30_H2_DF30_V01_03_26_2023.pkl'\n",
    "# file_name = 'W30_H2_DF30_V23_03_27_2023.pkl'\n",
    "# file_name = 'W30_H2_DF30_V45_03_27_2023.pkl'\n",
    "file_name = 'W30_H2_DF30_V67_03_27_2023.pkl'\n",
    "#SWING\n",
    "# file_name = 'W30_H3_DF24_V00_SWING_04_07_2023.pkl'\n",
    "#POSITION\n",
    "# file_name = 'W30_H4_DF1W_V00_POSITION_04_07_2023.pkl'\n",
    "\n",
    "\n",
    "file_path = os.path.join(data_path, file_name)\n",
    "\n",
    "# Open and Read file\n",
    "with open(os.path.join(data_path, file_name), 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"START STATIONARIZING DATA\")\n",
    "for label in [\"LONG\", \"SHORT\"]:\n",
    "    create_technical_indicator_data(label, loaded_dict)\n",
    "    \n",
    "print(\"DONE!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e0d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 'SCALP'\n",
    "# horizon = 'POSITION'\n",
    "# horizon = 'SWING'\n",
    "OUT_FILE = '{}_STATIONARY_67.pkl'.format(horizon, )\n",
    "with open(os.path.join(data_path, OUT_FILE), 'wb') as f:\n",
    "    pickle.dump(loaded_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40791688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
