{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb146a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Used/Edited: 5/11/2023\n",
    "\n",
    "# PURPOSE: \n",
    "# Train a model using the generated images for SWING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2139769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from typing import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import datetime as dt\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras import datasets,models,layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "import keras_tuner\n",
    "from keras_tuner import Hyperband, GridSearch\n",
    "# Tune model training\n",
    "# https://keras.io/guides/keras_tuner/getting_started/\n",
    "\n",
    "# It is generally not needed to tune the number of epochs because a built-in callback is \n",
    "# passed to model.fit() to save the model at its best epoch evaluated by the validation_data.\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 2\n",
    "tf.random.set_seed(seed)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8490c82",
   "metadata": {},
   "source": [
    "## Set Image Type Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET HERE\n",
    "\n",
    "horizon='SWING'\n",
    "# horizon='SWING_ST'\n",
    "# horizon='SWING_CUT'\n",
    "\n",
    "# image_type = 'GAF'\n",
    "# image_type = 'GAF_AGG'\n",
    "image_type = 'CS'\n",
    "# image_type = 'TI'\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "splits = 6\n",
    "val_size = 450\n",
    "\n",
    "#Time-series split\n",
    "tscv = TimeSeriesSplit(n_splits=splits, test_size=val_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e732b",
   "metadata": {},
   "source": [
    "## Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9609814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_paths_labels(path):\n",
    "    dataframes = []\n",
    "    for sub_folder in ['LONG', 'SHORT']:\n",
    "        images = glob.glob(path + '/{}/*.png'.format(sub_folder))  # Get path to images\n",
    "        dates = [dt.split('/')[-1].split('\\\\')[-1].split('.')[0].replace('_', '-') for dt in images]\n",
    "        data_slice = pd.DataFrame({'Images': images, 'Labels': [sub_folder] * len(images), 'Dates': dates})\n",
    "        data_slice['Dates'] = pd.to_datetime(data_slice['Dates'], format='%Y-%m-%d-%H-%M-%S')\n",
    "        dataframes.append(data_slice)\n",
    "    data = pd.concat(dataframes)\n",
    "    data.sort_values(by='Dates', inplace=True)\n",
    "    del data['Dates']\n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_model_name(k, horizon, image_type):\n",
    "    return 'model_' + horizon + '_' + image_type + '_' + str(k) + '.h5'\n",
    "\n",
    "\n",
    "def myModel(hp, tg, IMG_SIZE):\n",
    "    IMG_SHAPE = IMG_SIZE + (3,)\n",
    "    # tf.keras.applications.MobileNetV2\n",
    "    # tf.keras.applications.resnet_v2.ResNet50V2\n",
    "    base_model = tf.keras.applications.resnet_v2.ResNet50V2(input_shape=IMG_SHAPE,\n",
    "                                                  include_top=False,\n",
    "                                                  weights='imagenet')\n",
    "\n",
    "    # This feature extractor converts each 160x160x3 image into a 5x5x1280 block of features\n",
    "    # lets see what it does to an example batch of images\n",
    "\n",
    "    image_batch, label_batch = next(iter(tg))\n",
    "    feature_batch = base_model(image_batch)\n",
    "#     print(feature_batch.shape)\n",
    "\n",
    "    # Feature extraction\n",
    "    # in this step, you will freeze the convolutional base created from the prev\n",
    "    # step and to use as a feature extractor.\n",
    "    # additionally, you add a classifier on top of it and train the top-level classifier\n",
    "\n",
    "    # Freeze the convolutional base\n",
    "    # it is important to freeze the convolutional base before you compile and train the model.\n",
    "    # freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training.\n",
    "    # MobileNet V2 has many layers, so setting the entire model's trainable flag False will freeze all of them\n",
    "\n",
    "    base_model.trainable = False\n",
    "#     base_model.trainable = True\n",
    "\n",
    "    # base model architecture\n",
    "#     base_model.summary()\n",
    "\n",
    "    # Add a classification head\n",
    "\n",
    "    # to generate from the block of features, average over the spatial 5x5 location,\n",
    "    # using tf.keras.layers.GloabalAveragePooling2D layer to convert the features to a single 1280-element vector per image\n",
    "\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    feature_batch_average = global_average_layer(feature_batch)\n",
    "#     print(feature_batch_average.shape)\n",
    "\n",
    "    # apply tf.keras.layers.Dense layer to convert these features into a single prediction per image\n",
    "    # you dont need an activation function here because this prediction will be treated as a logit, or a raw prediction value\n",
    "    # positive numbers predict class 1, negative numbers predict class 0\n",
    "    regularization = hp.Choice('regularization', values=[1e-4, 1e-5])\n",
    "    prediction_layer = tf.keras.layers.Dense(units=1,\n",
    "                                             kernel_regularizer=regularizers.L1L2(l1=regularization, l2=regularization),\n",
    "                                             bias_regularizer=regularizers.L2(regularization),\n",
    "                                             activity_regularizer=regularizers.L2(regularization))\n",
    "    prediction_batch = prediction_layer(feature_batch_average)\n",
    "\n",
    "    # build a model by chaining together the data augmentation, rescaling, base_model, and feature\n",
    "    # extractor layers using Keras Functional API\n",
    "    # as prev mentioned, use training=False b/c model contains a BatchNormalization layer\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(255, 255, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = global_average_layer(x)\n",
    "    \n",
    "    drop_out_rate = hp.Float(\"drop_out_rate\", min_value=0, max_value=1, step=0.2)\n",
    "    x = tf.keras.layers.Dropout(drop_out_rate)(x)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    model = myModel(hp, train_generator, (255, 255))\n",
    "    # compile the model before training it.\n",
    "    # since there are two classes, use the tf.keras.losses.BinaryCrossentropy loss \n",
    "    # with from_logits=True since hte model provides a linear output\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=hp_learning_rate),\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy', tf.keras.metrics.BinaryCrossentropy(from_logits=True), tf.keras.metrics.AUC(from_logits=True)])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa95a4e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2762017",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.dirname(os.getcwd())\n",
    "IMAGES_PATH = os.path.join(PATH, 'TRAIN/{}/{}'.format(horizon, image_type))\n",
    "X = image_paths_labels(IMAGES_PATH)\n",
    "\n",
    "# first split train, test\n",
    "split_ratio = int(np.floor(0.8*len(X)))\n",
    "train = X[:split_ratio]\n",
    "test = X[split_ratio:]\n",
    "\n",
    "# Rescale images by 1./255\n",
    "train_validate_datagen = ImageDataGenerator(rescale=1/255)#, validation_split=0.3) # set validation split\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "IMG_SIZE = (255, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bd09f7",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835f273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### COMBINE & EDIT\n",
    "\n",
    "save_dir = PATH + '/saved_models/{}/'.format(horizon)\n",
    "TIMESTAMP = dt.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(tscv.split(train)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    VALIDATION_ACCURACY = []\n",
    "    VALIDATION_LOSS = []\n",
    "    BEST_HYPS = []\n",
    "    train_index = train_index[i*val_size:]\n",
    "    if i < 5:\n",
    "        continue\n",
    "    df_train = train.iloc[train_index]\n",
    "    df_val = train.iloc[val_index]\n",
    "\n",
    "    train_generator = train_validate_datagen.flow_from_dataframe(\n",
    "        dataframe=df_train,\n",
    "        directory=IMAGES_PATH,\n",
    "        target_size=IMG_SIZE,\n",
    "        x_col='Images',\n",
    "        y_col='Labels',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        class_mode='binary',\n",
    "    )\n",
    "    \n",
    "    validation_generator = train_validate_datagen.flow_from_dataframe(\n",
    "        dataframe=df_val,\n",
    "        directory=IMAGES_PATH,\n",
    "        target_size=IMG_SIZE,\n",
    "        x_col='Images',\n",
    "        y_col='Labels',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        class_mode='binary',\n",
    "    )\n",
    "\n",
    "    print(\"train datagenerator : {}\".format(train_generator.n))\n",
    "    print(\"validation datagenerator : {}\".format(validation_generator.n))\n",
    "    steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    validation_steps = validation_generator.n//validation_generator.batch_size # if you have validation data \n",
    "    \n",
    "    \n",
    "    # Create Callbacks\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(i, horizon, image_type),\n",
    "                                                  monitor='val_accuracy', verbose=1,\n",
    "                                                  save_best_only=True, mode='max')\n",
    "    \n",
    "    # create a callback to stop training early after reaching a certain value for the validation loss\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3,  min_delta=0.001, verbose=1, restore_best_weights=True, start_from_epoch=3)\n",
    "    \n",
    "    # https://keras.io/api/keras_tuner/tuners/base_tuner/#tuner-class\n",
    "    tuner = keras_tuner.GridSearch(\n",
    "        hypermodel = build_model,\n",
    "        objective=\"val_accuracy\",\n",
    "        seed=5,\n",
    "        max_trials=30,\n",
    "        overwrite=True,\n",
    "        directory=\"model_{}_{}\".format(horizon, image_type),\n",
    "        project_name=\"tune_hypermodel_{}\".format(i),)\n",
    "    \n",
    "    tuner.search(train_generator, \n",
    "                 epochs=EPOCHS, \n",
    "                 steps_per_epoch=steps_per_epoch,\n",
    "                 validation_data=validation_generator, \n",
    "                 validation_steps=validation_steps,\n",
    "                 callbacks=[stop_early])\n",
    "\n",
    "    # Get the optimal hyperparameters\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    BEST_HYPS.append(best_hps.values)\n",
    "    print(f\"\"\"\n",
    "    The hyperparameter search is complete. The optimal regularization is {best_hps.get('regularization')},the optimal learning rate for the optimizer\n",
    "    is {best_hps.get('learning_rate')}, and the optimal drop out rate is {best_hps.get('drop_out_rate')}.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    history = model.fit(train_generator, \n",
    "                        epochs=EPOCHS,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=validation_generator, \n",
    "                        validation_steps=validation_steps,\n",
    "                        callbacks=[checkpoint, stop_early])\n",
    "\n",
    "    # Load best model to evaluate performance\n",
    "    model.load_weights(os.path.join(save_dir, \"model_\"+ horizon + '_' + image_type + '_' + str(i) + \".h5\"))\n",
    "    scores = model.evaluate(validation_generator)\n",
    "    print(\"{0}s: {1:.2f}%\".format(model.metrics_names[1], scores[1]*100))\n",
    "    results = dict(zip(model.metrics_names, scores))\n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "\n",
    "    # Good practice to explicitly close each \n",
    "    # tensorflow session prior to starting a \n",
    "    # new one in a loop for memory considerations\n",
    "    tf.keras.backend.clear_session()\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    dict_results = {'VALIDATION_ACCURACY': VALIDATION_ACCURACY, \n",
    "                    'VALIDATION_LOSS': VALIDATION_LOSS,\n",
    "                    'BEST_HYPS': BEST_HYPS}\n",
    "    df = pd.DataFrame(dict_results)\n",
    "    df.to_csv('model_{0}_{1}/results_{0}_{1}_fold{2}.csv'.format(horizon, image_type, i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add854b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for split in range(splits):\n",
    "    result = pd.read_csv('model_{0}_{1}/results_{0}_{1}_fold{2}.csv'.format(horizon, image_type, split))[['VALIDATION_ACCURACY', 'VALIDATION_LOSS', 'BEST_HYPS']]\n",
    "    frames.append(list(result.iloc[0]))\n",
    "results_df = pd.DataFrame(frames, columns = ['VALIDATION_ACCURACY', 'VALIDATION_LOSS', 'BEST_HYPS'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d666f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d4294",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_ACCURACY = list(results_df.VALIDATION_ACCURACY)\n",
    "VALIDATION_LOSS = list(results_df.VALIDATION_LOSS)\n",
    "BEST_HYPS = list(results_df.BEST_HYPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5eff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "\n",
    "best_model_index = VALIDATION_ACCURACY.index(max(VALIDATION_ACCURACY))\n",
    "# load and evaluate a saved model\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# load model\n",
    "# https://keras.io/guides/serialization_and_saving/\n",
    "# model = load_model(save_dir + 'model_{}_{}{}.h5'.format(horizon, image_type, best_model_index), custom_objects={'mda': mda})\n",
    "model = load_model(save_dir + 'model_{}_{}_{}.h5'.format(horizon, image_type, best_model_index))\n",
    "\n",
    "# summarize model.\n",
    "model.summary()\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test,\n",
    "    x_col='Images',\n",
    "    y_col='Labels',\n",
    "    directory=IMAGES_PATH,\n",
    "    target_size=IMG_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "score = model.evaluate(test_generator, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_HYPS[best_model_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "print(\"{}: {}%\".format(model.metrics_names[0], score[0]*100))\n",
    "print(\"{}: {}%\".format(model.metrics_names[1], score[1]*100))\n",
    "print(\"{}: {}%\".format(model.metrics_names[2], score[2]*100))\n",
    "print(\"{}: {}%\".format(model.metrics_names[3], score[3]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d74e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making predictions with the model\n",
    "# Get the predicted values for the test set:\n",
    "# test_generator.filenames #LONG = 0, SHORT = 1\n",
    "\n",
    "y_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred = pd.DataFrame(np.column_stack((test_generator.labels, y_pred)), columns=[\"true\", \"predicted\"])\n",
    "true_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51843d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred.to_csv('../data/true_pred_2dCNN_{}_{}.csv'.format(horizon, image_type), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
