{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Used/Edited: 4/24/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: \n",
    "# Train a model using the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from typing import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import datetime as dt\n",
    "from tensorflow.keras import regularizers\n",
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import datasets,models,layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras_tuner\n",
    "from keras_tuner import Hyperband, GridSearch\n",
    "# Tune model training\n",
    "# https://keras.io/guides/keras_tuner/getting_started/\n",
    "\n",
    "# It is generally not needed to tune the number of epochs because a built-in callback is \n",
    "# passed to model.fit() to save the model at its best epoch evaluated by the validation_data.\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 2\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c04d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET HERE\n",
    "horizon = 'SCALP'\n",
    "# horizon='POSITION'\n",
    "image_type = 'GAF'\n",
    "# image_type = 'GAF_AGG'\n",
    "# image_type = 'CS'\n",
    "# image_type = 'TI'\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "splits = 3\n",
    "val_size = 150\n",
    "\n",
    "#Time-series split\n",
    "tscv = TimeSeriesSplit(n_splits=splits, test_size=val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab32f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#POSITION\n",
    "#GAF /Users/rurikoimai/Desktop/thesis/sample_pipeline/TRAIN/POSITION/GAF/LONG/2001_06_24_00_00_00.png\n",
    "#AGG GAF GAF_AGG/LONG/2001_06_24_00_00_00.png\n",
    "#CS CS/LONG/2001_06_24_00_00_00.png\n",
    "#TI\n",
    "img = mpimg.imread('/Users/rurikoimai/Desktop/thesis/sample_pipeline/TRAIN/POSITION/GAF_AGG/LONG/2001_06_24_00_00_00.png')\n",
    "imgplot = plt.imshow(img)\n",
    "print(\"size of image {}\".format(imgplot.get_size()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9609814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_paths_labels(path):\n",
    "    dataframes = []\n",
    "    for sub_folder in ['LONG', 'SHORT']:\n",
    "        images = glob.glob(path + '/{}/*.png'.format(sub_folder))  # Get path to images\n",
    "        dates = [dt.split('/')[-1].split('\\\\')[-1].split('.')[0].replace('_', '-') for dt in images]\n",
    "        data_slice = pd.DataFrame({'Images': images, 'Labels': [sub_folder] * len(images), 'Dates': dates})\n",
    "        data_slice['Dates'] = pd.to_datetime(data_slice['Dates'], format='%Y-%m-%d-%H-%M-%S')\n",
    "        dataframes.append(data_slice)\n",
    "    data = pd.concat(dataframes)\n",
    "    data.sort_values(by='Dates', inplace=True)\n",
    "    del data['Dates']\n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_model_name(k, horizon, image_type):\n",
    "    return 'model_' + horizon + '_' + image_type + '_' + str(k) + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2762017",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.dirname(os.getcwd())\n",
    "IMAGES_PATH = os.path.join(PATH, 'TRAIN/{}/{}'.format(horizon, image_type))\n",
    "X = image_paths_labels(IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first split train, test\n",
    "split_ratio = int(np.floor(0.8*len(X)))\n",
    "train = X[:split_ratio]\n",
    "test = X[split_ratio:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170162d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75235c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Labels'].value_counts(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale images by 1./255\n",
    "train_validate_datagen = ImageDataGenerator(rescale=1/255)#, validation_split=0.3) # set validation split\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "IMG_SIZE = (255, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1841638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myModel(hp, tg, IMG_SIZE):\n",
    "    IMG_SHAPE = IMG_SIZE + (3,)\n",
    "    # tf.keras.applications.MobileNetV2\n",
    "    # tf.keras.applications.resnet_v2.ResNet50V2\n",
    "    base_model = tf.keras.applications.resnet_v2.ResNet50V2(input_shape=IMG_SHAPE,\n",
    "                                                  include_top=False,\n",
    "                                                  weights='imagenet')\n",
    "\n",
    "    # This feature extractor converts each 160x160x3 image into a 5x5x1280 block of features\n",
    "    # lets see what it does to an example batch of images\n",
    "\n",
    "    image_batch, label_batch = next(iter(tg))\n",
    "    feature_batch = base_model(image_batch)\n",
    "#     print(feature_batch.shape)\n",
    "\n",
    "    # Feature extraction\n",
    "    # in this step, you will freeze the convolutional base created from the prev\n",
    "    # step and to use as a feature extractor.\n",
    "    # additionally, you add a classifier on top of it and train the top-level classifier\n",
    "\n",
    "    # Freeze the convolutional base\n",
    "    # it is important to freeze the convolutional base before you compile and train the model.\n",
    "    # freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training.\n",
    "    # MobileNet V2 has many layers, so setting the entire model's trainable flag False will freeze all of them\n",
    "\n",
    "    base_model.trainable = False\n",
    "#     base_model.trainable = True\n",
    "\n",
    "    # base model architecture\n",
    "#     base_model.summary()\n",
    "\n",
    "    # Add a classification head\n",
    "\n",
    "    # to generate from the block of features, average over the spatial 5x5 location,\n",
    "    # using tf.keras.layers.GloabalAveragePooling2D layer to convert the features to a single 1280-element vector per image\n",
    "\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    feature_batch_average = global_average_layer(feature_batch)\n",
    "#     print(feature_batch_average.shape)\n",
    "\n",
    "    # apply tf.keras.layers.Dense layer to convert these features into a single prediction per image\n",
    "    # you dont need an activation function here because this prediction will be treated as a logit, or a raw prediction value\n",
    "    # positive numbers predict class 1, negative numbers predict class 0\n",
    "    regularization = hp.Choice('regularization', values=[1e-4, 1e-5])\n",
    "    prediction_layer = tf.keras.layers.Dense(units=1,\n",
    "                                             kernel_regularizer=regularizers.L1L2(l1=regularization, l2=regularization),\n",
    "                                             bias_regularizer=regularizers.L2(regularization),\n",
    "                                             activity_regularizer=regularizers.L2(regularization))\n",
    "    prediction_batch = prediction_layer(feature_batch_average)\n",
    "\n",
    "    # build a model by chaining together the data augmentation, rescaling, base_model, and feature\n",
    "    # extractor layers using Keras Functional API\n",
    "    # as prev mentioned, use training=False b/c model contains a BatchNormalization layer\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(255, 255, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = global_average_layer(x)\n",
    "    \n",
    "    drop_out_rate = hp.Float(\"drop_out_rate\", min_value=0, max_value=1, step=0.2)\n",
    "    x = tf.keras.layers.Dropout(drop_out_rate)(x)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    model = myModel(hp, train_generator, (255, 255))\n",
    "    # compile the model before training it.\n",
    "    # since there are two classes, use the tf.keras.losses.BinaryCrossentropy loss \n",
    "    # with from_logits=True since hte model provides a linear output\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    threshold = hp.Choice('thresholds', values=[0.4, 0.5, 0.6, 0.7])\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=hp_learning_rate),\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy', tf.keras.metrics.BinaryCrossentropy(from_logits=True), tf.keras.metrics.AUC(from_logits=True)])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMBINE & EDIT\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "BEST_HYPS = []\n",
    "save_dir = PATH + '/saved_models/{}/'.format(horizon)\n",
    "TIMESTAMP = dt.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(tscv.split(train)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    \n",
    "    train_index = train_index[i*val_size:]\n",
    "#     if i == 5:\n",
    "#         break\n",
    "    df_train = train.iloc[train_index]\n",
    "    df_val = train.iloc[val_index]\n",
    "\n",
    "    train_generator = train_validate_datagen.flow_from_dataframe(\n",
    "        dataframe=df_train,\n",
    "        directory=IMAGES_PATH,\n",
    "        target_size=IMG_SIZE,\n",
    "        x_col='Images',\n",
    "        y_col='Labels',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        class_mode='binary',\n",
    "    )\n",
    "    \n",
    "    validation_generator = train_validate_datagen.flow_from_dataframe(\n",
    "        dataframe=df_val,\n",
    "        directory=IMAGES_PATH,\n",
    "        target_size=IMG_SIZE,\n",
    "        x_col='Images',\n",
    "        y_col='Labels',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        class_mode='binary',\n",
    "    )\n",
    "\n",
    "    print(\"train datagenerator : {}\".format(train_generator.n))\n",
    "    print(\"validation datagenerator : {}\".format(validation_generator.n))\n",
    "    steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    validation_steps = validation_generator.n//validation_generator.batch_size # if you have validation data \n",
    "    \n",
    "    \n",
    "    # Create Callbacks\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(i, horizon, image_type),\n",
    "                                                  monitor='val_accuracy', verbose=1,\n",
    "                                                  save_best_only=True, mode='max')\n",
    "    \n",
    "    # create a callback to stop training early after reaching a certain value for the validation loss\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3,  min_delta=0.001, verbose=1, restore_best_weights=True, start_from_epoch=3)\n",
    "    \n",
    "    # https://keras.io/api/keras_tuner/tuners/base_tuner/#tuner-class\n",
    "    tuner = keras_tuner.GridSearch(\n",
    "        hypermodel = build_model,\n",
    "        objective=\"val_accuracy\",\n",
    "        seed=5,\n",
    "        max_trials=30,\n",
    "        overwrite=True,\n",
    "        directory=\"model_{}_{}\".format(horizon, image_type),\n",
    "        project_name=\"tune_hypermodel_{}\".format(i),)\n",
    "    \n",
    "    tuner.search(train_generator, \n",
    "                 epochs=EPOCHS, \n",
    "                 steps_per_epoch=steps_per_epoch,\n",
    "                 validation_data=validation_generator, \n",
    "                 validation_steps=validation_steps,\n",
    "                 callbacks=[stop_early])\n",
    "\n",
    "    # Get the optimal hyperparameters\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    BEST_HYPS.append(best_hps.values)\n",
    "    print(f\"\"\"\n",
    "    The hyperparameter search is complete. The optimal regularization is {best_hps.get('regularization')},the optimal learning rate for the optimizer\n",
    "    is {best_hps.get('learning_rate')}, the optimal drop out rate is {best_hps.get('drop_out_rate')}, and the best threshold is {best_hps.get('thresholds')}.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    history = model.fit(train_generator, \n",
    "                        epochs=EPOCHS,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=validation_generator, \n",
    "                        validation_steps=validation_steps,\n",
    "                        callbacks=[checkpoint, stop_early])\n",
    "\n",
    "    # Load best model to evaluate performance\n",
    "    model.load_weights(os.path.join(save_dir, \"model_\"+ horizon + '_' + image_type + '_' + str(i) + \".h5\"))\n",
    "    scores = model.evaluate(validation_generator)\n",
    "    print(\"{0}s: {1:.2f}%\".format(model.metrics_names[1], scores[1]*100))\n",
    "    results = dict(zip(model.metrics_names, scores))\n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "\n",
    "    # Good practice to explicitly close each \n",
    "    # tensorflow session prior to starting a \n",
    "    # new one in a loop for memory considerations\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    \n",
    "dict_results = {'VALIDATION_ACCURACY': VALIDATION_ACCURACY, \n",
    "                'VALIDATION_LOSS': VALIDATION_LOSS,\n",
    "                'BEST_HYPS': BEST_HYPS}\n",
    "df = pd.DataFrame(dict_results)\n",
    "df.to_csv('results_{}_{}.csv'.format(horizon, image_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curves\n",
    "\n",
    "# take a look at the learning curves of the training and validation accuracy/loss\n",
    "# when using the MobileNetV2 base model as a fixed feature extractor\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11654efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('./results_{}_{}.csv'.format(horizon, image_type))[['VALIDATION_ACCURACY', 'VALIDATION_LOSS', 'BEST_HYPS']]\n",
    "\n",
    "VALIDATION_ACCURACY = list(results_df.VALIDATION_ACCURACY)\n",
    "VALIDATION_LOSS = list(results_df.VALIDATION_LOSS)\n",
    "BEST_HYPS = list(results_df.BEST_HYPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5eff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "\n",
    "best_model_index = VALIDATION_ACCURACY.index(max(VALIDATION_ACCURACY))\n",
    "# load and evaluate a saved model\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# load model\n",
    "# https://keras.io/guides/serialization_and_saving/\n",
    "# model = load_model(save_dir + 'model_{}_{}{}.h5'.format(horizon, image_type, best_model_index), custom_objects={'mda': mda})\n",
    "model = load_model(save_dir + 'model_{}_{}_{}.h5'.format(horizon, image_type, best_model_index))\n",
    "\n",
    "# summarize model.\n",
    "model.summary()\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test,\n",
    "    x_col='Images',\n",
    "    y_col='Labels',\n",
    "    directory=IMAGES_PATH,\n",
    "    target_size=IMG_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "score = model.evaluate(test_generator, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df086db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_HYPS[best_model_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "print(\"{}: {}%\".format(model.metrics_names[0], score[0]*100))\n",
    "print(\"{}: {}%\".format(model.metrics_names[1], score[1]*100))\n",
    "print(\"{}: {}%\".format(model.metrics_names[2], score[2]*100))\n",
    "print(\"{}: {}%\".format(model.metrics_names[3], score[3]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d74e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making predictions with the model\n",
    "# Get the predicted values for the test set:\n",
    "# test_generator.filenames #LONG = 0, SHORT = 1\n",
    "\n",
    "y_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.column_stack((test_generator.labels, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred = pd.DataFrame(np.column_stack((test_generator.labels, y_pred)), columns=[\"true\", \"predicted\"])\n",
    "true_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.labels.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(true_pred['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51843d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred.to_csv('../data/true_pred_2dCNN_{}_{}.csv'.format(horizon, image_type), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98379ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf02b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_half = int(len(train)*0.6)\n",
    "# train_test = train[split_half:]\n",
    "train_test = train[-15000:]\n",
    "len(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how many labels for each class?\n",
    "# splits = 3\n",
    "# val_size = 150\n",
    "\n",
    "#check class\n",
    "# train_generator.class_indices\n",
    "\n",
    "splits = 10\n",
    "max_train_size=5000\n",
    "val_size = 1000\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=splits, max_train_size=max_train_size, test_size=val_size)\n",
    "\n",
    "# for i, (train_index, val_index) in enumerate(tscv.split(train)):\n",
    "for i, (train_index, val_index) in enumerate(tscv.split(train_test)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    \n",
    "#     train_index = train_index[i*val_size:]\n",
    "        \n",
    "    print(f\" Train: start datetime={train_test.iloc[train_index[0], 0][-30:]}\")\n",
    "    print(f\" Train: end datetime={train_test.iloc[train_index[-1], 0][-30:]}\")\n",
    "    print(f\" Validation start datetime={train_test.iloc[val_index[0], 0][-30:]}\")\n",
    "    print(f\" Validation end datetime={train_test.iloc[val_index[-1], 0][-30:]}\")\n",
    "\n",
    "#     print(train_index)\n",
    "#     print(len(train_index))\n",
    "#     print(val_index)\n",
    "#     print(len(val_index))\n",
    "    \n",
    "#     df_train = train.iloc[train_index]\n",
    "    df_train = train_test.iloc[train_index]\n",
    "    df_val = train_test.iloc[val_index]\n",
    "\n",
    "    train_generator = train_validate_datagen.flow_from_dataframe(\n",
    "        dataframe=df_train,\n",
    "        directory=IMAGES_PATH,\n",
    "        target_size=IMG_SIZE,\n",
    "        x_col='Images',\n",
    "        y_col='Labels',\n",
    "        shuffle=False,\n",
    "        class_mode='binary',\n",
    "    )\n",
    "    \n",
    "    validation_generator = train_validate_datagen.flow_from_dataframe(\n",
    "        dataframe=df_val,\n",
    "        directory=IMAGES_PATH,\n",
    "        target_size=IMG_SIZE,\n",
    "        x_col='Images',\n",
    "        y_col='Labels',\n",
    "        shuffle=False,\n",
    "        class_mode='binary',\n",
    "    )\n",
    "    \n",
    "    print(f'train LONG :{train_generator.labels.count(0)}')\n",
    "    print(f'train SHORT :{train_generator.labels.count(1)}')\n",
    "    print(f'validation LONG :{validation_generator.labels.count(0)}')\n",
    "    print(f'validation SHORT: {validation_generator.labels.count(1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbaf0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75785be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4694473",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef8714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
